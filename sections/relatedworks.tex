\section{Background Information}
    \subsection{Bias in \gls{ml}}
        The progress and the commercial value of \gls{ml} is exciting; however, the trust of society in \gls{ml} will take time to build due to inherent biases. The exact definitions and implications of bias vary between sources, as do its sources and types. A common theme is that bias hinders performance ratings in ways that skew to a particular sub-population-- the source varies, whether from humans~\cite{windmann1998subconscious}, data and label types~\cite{tommasi2017deeper}, \gls{ml} models~\cite{amini2019uncovering, kim2019learning}, or evaluation protocols~\cite{stock2018convnets}. For instance, a vehicle-detection model might miss cars if training data was mostly trucks. In practice, many \gls{ml} systems learn on biased data, which could be detrimental for society. 


    \subsection{Bias in \gls{fr}}
        Problems of bias in \gls{fr} have been driven by different motivations; therefore, solutions have been posed for different matters. To name a few: data augmentation~\cite{yin2019feature}, one-shot learning~\cite{ding2018one}, demographic parity and fairness with priority on privacy~\cite{huang2018generative}, domain adaptation~\cite{wang2018racial}, differences in face-based attributes across demographics~\cite{wang2018they}, and even data exploration~\cite{muthukumar2019}. Yin~\etal proposed to augment the feature space of underrepresented classes using other classes with a diverse collection of samples to encourage distributions of underrepresented classes to more closely resemble that of the others~\cite{yin2019feature}. Similarly, others formulated the imbalanced class problem as one-shot learning, where a \gls{gan} was trained to generate face features to augment classes with fewer samples~\cite{ding2018one}. \gls{gapf} was proposed to create fair representations of the data in a quantifiable way, allowing for the finding of a de-correlation scheme from the data without access to its statistics~\cite{huang2018generative}. Wang~\etal defined subgroups at a finer-level (\ie Chinese, Japanese, Korean), and determined the familiarity of faces inter-subgroup~\cite{wang2018they}. Genders have also been used to make subgroups (\eg for analysis of gender-based face encodings~\cite{muthukumar2019}). Most recently,~\cite{wang2018racial} proposed adapting domains to bridge the bias gap by knowledge transfer, which was supported by a novel data collection, \gls{rfw}. The release of \gls{rfw} occurred after \gls{bfw} was built - although similar in terms of demographics, \gls{rfw} uses faces from MSCeleb~\cite{guo2016ms} for testing, and assumes CASIA-Face~\cite{yi2014learning} and VGG2~\cite{Cao18} were used to train. In contrast, our \gls{bfw} assumes VGG2 as the test set. 
        % As shown in \cite{wang2018racial}, MSCeleb is highly imbalanced, primarily consisting of images of white individuals. 
        % For this, we expect bias from data, for this is the training set. 
        Furthermore, \gls{bfw} balances gender and race: \gls{rfw} splits subgroups by gender and race, while \gls{bfw} has gender, race, or both). 
        % Thus, \gls{rfw} and \gls{bfw} complement one another. % - both add demographic for subjects with faces in renowned, large-scale \gls{fr} datasets.

    
Most similar to us is~\cite{das2018, demogPairs, lopez2019dataset, srinivas2019face} - each were motivated by insufficient paired data for studying bias in \gls{fr}; then, problems were addressed using labeled data from existing image collections. Specifically, Hupont~\etal curated a set of faces based on racial demographics (\ie \gls{a}, \gls{b}, and \gls{w}) called \gls{dp}~\cite{demogPairs}, while~\cite{srinivas2019face} honed in on adults versus children called \gls{itwcc}. Like the proposed \gls{bfw}, both were built by sampling existing databases, but with the addition of tags for the respective subgroups of interest. Besides, the additional data of \gls{bfw} (\ie added an additional subgroup \gls{i}, along with additional subjects with more faces for all subgroups), we also further split subgroups by gender. Furthermore, we focus on the problem of facial verification and the different levels of sensitivity in cosine similarity scores per subgroup.

    \begin{table*}[t!]
        
        \centering
        \caption{\small{\textbf{\gls{bfw} compared to related datasets.} Our \gls{bfw} is balanced across ID, gender, and ethnicity (Table~\ref{tab:ethnic-splits}). Compared with \gls{dp}, \gls{bfw} provides more samples per subject and subgroups per set, while only using a single resource, VGG2. \gls{rfw}, on the other hand, supports a different task (\ie domain adaptation), and focuses on race-distribution, but not the distribution of identities.}}
        \scriptsize
        \begin{tabular}{rccccccc}%x{10mm}x{3mm}x{10mm}x{8mm}}%\toprule
        
            \multicolumn{2}{c}{Database} & \multicolumn{3}{c}{Number of}& \multicolumn{3}{c}{Balanced Labels}\\
            \cmidrule(lr){1-2}	\cmidrule(lr){3-5} \cmidrule(lr){6-8}
            Name & Source Data & Faces &  IDs & Subgroups & ID & Ethnicity & Gender\\\midrule
            \gls{dp}~\cite{demogPairs}     & CASIA-W~\cite{yi2014learning}, VGG~\cite{schroff2015facenet} \&VGG2~\cite{Cao18} & 10,800& 600 & 6 &\checkc& \checkc &\checkc \\
            \gls{rfw}~\cite{wang2018racial}     &  MS-Celeb-1M &$\approx$80,000&$\approx$12,000& 4 & \xmark & \checkc &\xmark \\
            \gls{bfw} (ours) & VGG2 & 20,000 & 800 &8 & \checkc & \checkc &\checkc \\\bottomrule
        \end{tabular}
        \label{tab:compared}
            \vspace{-5mm}
    \end{table*}
    
\subsection{Human bias in \gls{ml}}
Bias is not unique to \gls{ml} - humans are also susceptible to a perceived bias. In fact, biases exist across race, gender, and even age~\cite{10.1007/978-3-030-13469-3_68, bar2006, meissner2001, nicholls2018}. Wang~\etal showed machines surpass human performance in discriminating between Japanese, Chinese, or Korean faces by nearly 150\%~\cite{wang2018they}, as humans just pass random (\ie 38.89\% accuracy).

We expect human bias to skew to their own genders and races. For this, we measure the human perception with face pairs of different subgroups (Section~\ref{subsec:human-assessment}). The results concur with~\cite{wang2018they}, as we also recorded overall averages below random ($<$50\%). %The details on settings and number of submissions per demographics are discussed in Section~\ref{}.
