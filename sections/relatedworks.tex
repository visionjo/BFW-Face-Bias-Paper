
\section{Background Information}
\subsection{Bias in \gls{ml}}
Although the community is excited about the progress and the commercial value of \gls{ml}, the trust between the society and techniques will take time to build due to the bias in algorithms. The exact definitions and implications of bias tend to vary between sources, as do its sources and types. One common theme is the tendency of bias to hinder performance ratings in ways that skew in favor of a particular subpopulation. At a high-level, bias can be introduced by humans~\cite{windmann1998subconscious}, data and label types~\cite{tommasi2017deeper}, \gls{ml} models~\cite{amini2019uncovering, kim2019learning}, and during evaluation~\cite{stock2018convnets}. For instance, a vehicle-detection model might miss cars if training set was mostly trucks. In practice, many \gls{ml} systems learn on biased datasets, which could harm our society. 


% \vspace{-5pt}
\subsection{Bias in \gls{fr}}
Problems of bias in \gls{fr} have been driven by different motivations and have thereforem been solved by different means. To name a few: problems of data augmentation~\cite{yin2019feature}, one-shot learning~\cite{ding2018one}, demographic parity and fairness with priority on privacy~\cite{huang2018generative}, domain adaptation~\cite{wang2018racial}, differences in face-based attributes across demographics~\cite{wang2018they}, and even exploratory data analysis~\cite{muthukumar2019}. Yin et al. \cite{yin2019feature} proposed to augment the feature space of underrepresented classes using other classes with a diverse collection of samples to encourage distributions of underrepresented classes to more closely resemble that of the others. Similarly,~\cite{ding2018one} formulated the imbalanced class as one-shot learning and trained a \gls{gan} to generate face features as a means of augmenting class with as few as one sample. \cite{huang2018generative} proposed \gls{gapf} to create fair representations of the data in a quantifiable way, allowing for the finding of a decorrelation scheme from the data without access to its statistics. Wang et al. \cite{wang2018they} defined subgroups at a finer-level (\ie Chinese, Japanese, Korean) to determine the familiarity of faces across these subgroups. Genders have also been used as subgroups for work in bias, whether for efforts of analysis and understanding of gender-based face encodings~\cite{muthukumar2019}. Most recently,~\cite{wang2018racial} proposed adapting domains to bridge the gap between races by knowledge transfer, which was supported by a novel labeled data collection, \gls{rfw}. The release of \gls{rfw} came after the completion of \gls{bfw}. Although similar in terms of demographics (\ie Asian, Black, Indian, and White), \gls{rfw} uses faces from MSCeleb~\cite{guo2016ms} based on the assumption CASIA-Face~\cite{yi2014learning} and VGG2~\cite{Cao18} was used for training. In contrast, our \gls{bfw} samples from VGG2 and assumes MSCeleb was the training set. As shown in \cite{wang2018racial}, MSCeleb is highly imbalanced, primarily consisting of images of white individuals. For this, we expect bias from data, for this is the training set. Furthermore, our experiments allow for the parsing of results based on gender demographics (\ie although \gls{rfw} does separate males and females in each subgroup, no consideration for splits in just gender or just races are incorporated into the experimental design). Thus, \gls{rfw} and \gls{bfw} are complementary, with both adding metadata on demographics for subjects with faces in renowned, large-scale \gls{fr} datasets.

Most similar to our work are the recent efforts in~\cite{das2018, demogPairs, lopez2019dataset, srinivas2019face}. A common factor of these efforts are claims of insufficient data supply to support studies on bias in \gls{fr}, and the introduction of new metadata on demographics for image sets parsed out of existing collections. Specifically, \cite{demogPairs} curated a set of faces based on racial demographics (\ie Asian, Black, and White) called \gls{dp}, while~\cite{srinivas2019face} honed in on adults versus children called gls{itwcc}. Like the proposed \gls{bfw}, both were built by sampling existing databases, but with the addition of tags for the respective subgroups of interest. Besides, the additional data of \gls{bfw} (\ie added an additional subgroup \gls{i}, along with additional subjects with more faces for all subgroups), we also further split subgroups by gender. Furthermore, we focus on the problem of facial verification and the different levels of sensitivity in cosine similarity scores per subgroup.

% \vspace{-5pt}
\subsection{Human bias in \gls{ml}}
Bias is not unique to \gls{ml}, as humans also susceptible to a perceived bias across demographics: it exists across different races, genders, and even ages~\cite{10.1007/978-3-030-13469-3_68, bar2006, meissner2001, nicholls2018}. \cite{wang2018they} showed machines surpass human performance in classifying faces as Japanese, Chinese, or Korean by nearly 150\%. Precisely, humans barely pass random with 
38.89\% accuracy (\ie $\frac{1}{3}$ is random).

We expect human bias to skew results in favor of their genders and races. For this, we measure the human perception of faces across demographics in a controlled experiment. In the end, the results concur~\cite{wang2018they}-- we too observed an overall average below random (\ie $<$50\%). Furthermore, we provide details on settings and counts on the number of submissions per demographics while analyzing per demographic.


