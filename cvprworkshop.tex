\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{sidecap}
\usepackage{wrapfig}
\usepackage[subrefformat=parens,labelformat=parens]{subcaption} %% provides subfigure
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{float}% If comment this, figure moves
\usepackage{geometry}

% \usepackage{FG2019}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage[acronym]{glossaries}
\usepackage{acronym}
\usepackage{collcell}
\newcommand{\minitab}[2][c]{\begin{tabular}{#1}#2\end{tabular}}

% symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% % \usepackage{hyperref}       % hyperlinks
% \usepackage[colorlinks = true,
%             linkcolor = blue,
%             urlcolor  = blue,
%             citecolor = blue,
%             anchorcolor = blue]{hyperref}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
            
% \usepackage{mathptmx} % assumes new font selection scheme installed
% \usepackage{lipsum}
% \usepackage{slashbox}
\usepackage{booktabs}       % professional-quality tables
%\usepackage{enumitem}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\usepackage[font=itshape]{quoting}
\usepackage{lipsum}
% \usepackage{subfig}
%% add
\usepackage{graphics}
\usepackage{xcolor}
\usepackage{color, colortbl}
\usepackage{comment}
\usepackage{booktabs, ragged2e}
% \usepackage{arydshln}
% \usepackage{todonotes}
% \usepackage{ctable}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{balance}

\usepackage{pifont}% http://ctan.org/pkg/pifont


\graphicspath{{figures/}} %Setting the graphicspath
\usepackage{bibunits}

\defaultbibliography{bias}
\defaultbibliographystyle{ieee_fullname}

\usepackage[acronym]{glossaries}
\usepackage{acronym}


\usepackage{array}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}
\newcommand{\ie}{\textit{i}.\textit{e}., }
\newcommand{\eg}{\textit{e}.\textit{g}., }
\newcommand*{\etc}{etc.\@\xspace}

\newcommand{\xmark}{\ding{56}}%
\newcommand{\checkc}{\ding{51}}%
\newcommand{\NA}{---}

\newacronym{ml}{ML}{machine learning}
\newacronym{fr}{FR}{facial recognition}
\newacronym{fv}{FV}{facial verification}

\newacronym{cnn}{CNN}{convolutional neural network}
\newacronym{nn}{NN}{neural network}
\newacronym{mtcnn}{MTCNN}{\emph{multi-task \gls{cnn}}}

\newacronym{gan}{GAN}{generative adversarial network}
\newacronym{se}{SE}{\emph{Squeeze-and-Excitation}}
\newacronym{d}{$D$}{discriminator}
\newacronym{g}{$G$}{generator}
\newacronym{dbvae}{DB-VAE}{Debiasing Variational Autoencoder}


\newacronym{lut}{LUT}{Look-Up-Table}
\newacronym{soa}{SOTA}{state-of-the-art}

\newacronym{fiw}{FIW}{Families In the Wild}
\newacronym{lfw}{LFW}{Labeled Faces in the Wild}
\newacronym{bfw}{BFW}{Balanced Faces In the Wild}
\newacronym{rfw}{RFW}{Racial Faces in-the-Wild:}
\newacronym{dp}{DemogPairs}{Demographic Pairs}
\newacronym{itwcc}{ITWCC}{Wild Child Celebrity}

\newacronym{m}{M}{\textit{Male}}
\newacronym{f}{F}{\textit{Female}}
\newacronym{a}{A}{\textit{Asian}}
\newacronym{b}{B}{\textit{Black}}
\newacronym{i}{I}{\textit{Indian}}
\newacronym{w}{W}{\textit{White}}
\newacronym{af}{AF}{\textit{Asian}-\textit{Female}}
\newacronym{am}{AM}{\textit{Asian}-\textit{Male}}
\newacronym{bf}{BF}{\textit{Black}-\textit{Female}}
\newacronym{bm}{BM}{\textit{Black}-\textit{Male}}
\newacronym{if}{IF}{\textit{Indian}-\textit{Female}}
\newacronym{im}{IM}{\textit{Indian}-\textit{Male}}
\newacronym{wf}{WF}{\textit{White}-\textit{Female}}
\newacronym{wm}{WM}{\textit{White}-\textit{Male}}


\newacronym{fd}{FD}{Face Discrimination}
\newacronym{bb}{BB}{bounding box}

\newacronym{sdm}{SDM}{signal detection model}
\newacronym{roc}{ROC}{receiver operating characteristic}
\newacronym{nmse}{NMSE}{Normalized Mean Square Error}
\newacronym{det}{DET}{Detection error trade-off}
\newacronym{tp}{TP}{true-positive}
\newacronym{fp}{FP}{false-positive}

\newacronym{tpir}{TPIR}{true-positive identification rate}
\newacronym{frir}{FRIR}{false-reject identification rate}
\newacronym{fpir}{FRIR}{false-positive identification rate}

\newacronym{fn}{FN}{false-negative}
\newacronym{frr}{FRR}{false-reject rate}
\newacronym{fnr}{FNR}{false-negative rate}

\newacronym{fpr}{FPR}{false-positive rate}

\newacronym{tpr}{TPR}{true-positive rate}


\newacronym{tar}{TAR}{True Acceptance Rate}
\newacronym{far}{FAR}{False Acceptance Rate}
\newacronym{eer}{EER}{Equal Error Rate}

\newacronym{cs}{CS}{Cosine Similarity}


\newacronym{lime}{LIME}{Local Interpretable Model-Agnostic Explanations}
\newacronym{nas}{NAS}{Neural Architecture Search}
\newacronym{gapf}{GAPF}{Generative Adversarial Privacy and Fairness}

                                                         % paper
%\setlist[itemize]{leftmargin=*}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand{\mc}[2]{\multicolumn{#1}{c}{#2}}
\definecolor{Gray}{gray}{0.85}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{b}{>{\columncolor{white}}c}
\renewcommand{\arraystretch}{1.4}
%\newcommand{\xmark}{\ding{53}}%

\newcommand{\highlightb}[1]{%
\colorbox{blue!30}{$\displaystyle#1$}}
\newcommand{\vo}{\vec{o}\@ifnextchar{^}{\,}{}}

\newcommand{\vx}{\vec{x}\@ifnextchar{^}{\,}{}}

% \let\vec\mathbf
% \usepackage[T1]{fontenc}
% \usepackage[latin9]{inputenc}

\def\colorModel{hsb} %You can use rgb or hsb
\usepackage{babel}
% \usepackage[table]{xcolor}
% 
% \def\colorModel{hsb} %You can use rgb or hsb

\usepackage{hhline}
\newcommand\ColCell[1]{
  \pgfmathparse{#1<50?1:0}  %Threshold for changing the font color into the cells
    \ifnum\pgfmathresult=0\relax\color{white}\fi
  \pgfmathsetmacro\compA{0}      %Component R or H
  \pgfmathsetmacro\compB{#1/100} %Component G or S
  \pgfmathsetmacro\compC{1}      %Component B or B
  \edef\x{\noexpand\centering\noexpand\cellcolor[\colorModel]{\compA,\compB,\compC}}\x #1
  } 
\newcolumntype{E}{>{\collectcell\ColCell}m{0.4cm}<{\endcollectcell}}  %Cell width
\newcommand*\rot{\rotatebox{90}}

\cvprfinalcopy % *** Uncomment this line for the final submission

% \def\cvprPaperID{0001} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Face Recognition: Too Bias, or Not Too Bias?}

%use this in case of several affiliations
\author{\parbox{16cm}{\centering
    {\large Joseph P Robinson$^1$, Gennady Livitz$^2$, Yann Henon$^2$, Can Qin$^1$,\\ Yun Fu$^1$, and Samson Timoner$^2$}\\
    {\normalsize
    \hspace{-.4in}$^{1}$Northeastern University\hspace{.7in} $^{2}$ISM Connect}}
    % \thanks{}% <-this % stops a space
}


\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
We reveal critical insights into problems of bias in state-of-the-art \gls{fr} systems using a novel \gls{bfw} dataset: data balanced for gender and ethnic groups. We show variations in the optimal scoring threshold for face-pairs across different subgroups. Thus, the conventional approach of learning a  global threshold for all pairs resulting in performance gaps among subgroups. By learning subgroup-specific thresholds, we not only mitigate problems in performance gaps but also show a notable boost in the overall performance. Furthermore, we do a human evaluation to measure the bias in humans, which supports the hypothesis that such a bias exists in human perception. For the \gls{bfw} database, source code, and more, visit \href{https://github.com/visionjo/facerec-bias-bfw}{https://github.com/visionjo/facerec-bias-bfw}.
% \href{https://github.com/visionjo/facerec-bias-bfw}{github.com/visionjo/facerec-bias-bfw}
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sections/introduction}
\input{sections/relatedworks}

\begin{figure}[t!]
    \centering
    \includegraphics[width=.8\linewidth]{figures/montage.pdf}
    \caption{\small{\textbf{\gls{bfw}.} Average face of the different subsets: \emph{top-left}: the entire \gls{bfw}; \emph{top-row} per race;  \emph{left-column}: per gender. The others represent the ethnicity and gender of the race and gender, respectively. Table~\ref{tab:ethnic-splits} defines the acronyms of subgroups.}}
    \label{fig:avg-faces}
\end{figure}


\glsunset{if}\glsunset{im}\glsunset{af}\glsunset{am}\glsunset{bf}\glsunset{bm}\glsunset{wf}\glsunset{wm}
\input{sections/section3}



\begin{figure}[t!]
       \centering
    \includegraphics[width=.95\linewidth]{figures/fpr_percent_diff-crop.pdf}
    \caption{\small{\textbf{Percent difference from intended \gls{fpr}.} \emph{Top:} $t_g$ yields \gls{fpr} the span as large as 2$\times$ (\ie 200\%) that intended (\ie \gls{wm} for 1e-4). Furthermore, \gls{f} subgroups tend to perform worse than intended for all cases (while \gls{m} tend to overshoot intended performance, with exception of \gls{im} in for \gls{fpr}=1e-4). \emph{Bottom:} Subgroup-specific thresholds reduces this difference to near zero, where there are small differences, the percent difference across different subgroups is fair (\ie FPR=1e-4).}}\label{fig:percent:difference}
\end{figure}

\input{sections/analysis}

%  for it better adapts wh specific to subgroup) shift in sensitivity across subgroups when deep encodings are compared, the underlying mechanism 

\begin{figure}[t!] 
	\centering    
	\includegraphics[width=.55\linewidth] {figures/human_eval.pdf}
		\caption{\small{\textbf{Human assessment (qualitative).} $\checkmark$ for \emph{match}; $\times$ for \emph{non-match}. Accuracy scores shown as bar plots. Humans are most successful at recognizing their own subgroup, with a few exceptions (\eg bottom).}}
		\label{fig:human-eval} 
\end{figure} 


\glsresetall
\section{Conclusion}
We introduce a new data set \gls{bfw} with eight subgroups balanced across gender and ethnicity. With this, and upon highlighting the challenges and shortcomings of grouping subjects as a single subset, we provide evidence that forming subgroups is meaningful, as the FR algorithm rarely makes mistakes across subgroups. We used an \textit{off-the-shelf} Sphereface, hypothesizing this SOTA CNN suffers from bias because of the imbalanced train-set. Once established that the results do suffer from problems of bias, we observed that the same threshold across ethnic and gender subgroups leads to differences in the \gls{fpr} up to a factor of two. Furthermore, the percent difference from in which is seemingly the cause of the frenzy about bias in FR in main-stream media. Furthermore, we ameliorate these differences with a per-subgroup threshold, leveling out FPR, and achieving a higher TPR. We hypothesized that most humans grew amongst more than their own demographic and, therefore, effectively learn from imbalanced datasets-- a human evaluation validated that humans are biased, as most recognized their personal demographic best. The focused research findings presented here, along with the public database included, are extendable in vast ways. Thus, we see this as the slither to a much larger problem of bias in ML.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{references}
}

% \putbib
% \end{bibunit}
% \begin{bibunit}
% \input{supplemental.tex}
% \end{bibunit}

\end{document}
